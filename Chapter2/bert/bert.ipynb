{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1. Dataset preparation\n",
    "\n",
    "The common large-scale data sets for pre-training language models can be downloaded and loaded directly in the Dataset library. For example, if the English corpus of Wikipedia is used, the data can be obtained directly through the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 41/41 [00:36<00:00,  1.12files/s]\n",
      "Generating train split: 100%|██████████| 6458670/6458670 [00:40<00:00, 157701.13 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 72416608\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 8046290\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = 'E:\\\\huggingface_model'\n",
    "os.environ[\"HF_HOME\"] = 'E:\\\\huggingface_model'\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = 'E:\\\\huggingface_model'\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'E:\\\\huggingface_model'\n",
    "\n",
    "# 确保目录存在\n",
    "cache_dir = 'E:\\\\huggingface_model'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# 导入需要的库\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "# 加载数据集\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\", cache_dir=cache_dir, trust_remote_code=True)\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", cache_dir=cache_dir, trust_remote_code=True)\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != 'text'])\n",
    "\n",
    "# 合并数据集\n",
    "dataset = concatenate_datasets([bookcorpus, wiki])\n",
    "\n",
    "# 划分训练集和测试集\n",
    "d = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# 打印一些信息来确认\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the training and test data are saved in the local file respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_text(dataset, output_filename = \"data.txt\"):\n",
    "    with open(output_filename, 'w') as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)\n",
    "\n",
    "dataset_to_text(d[\"train\"], \"train.txt\")\n",
    "dataset_to_text(d[\"test\"], \"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2. Training the Tokenizer\n",
    "\n",
    "BERT use wordpiece word segmentation to determine whether to cut a complete word into multiple lexical elements according to the word frequency in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "\n",
    "files = [\"train.txt\"]\n",
    "vocab_size = 30_522\n",
    "max_length = 512\n",
    "truncate_longer_sample = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
