# 1. The LLM architeture

## Outline

* **High-level view**: Revisit the encoder-decoder Transformer architecture, and more specifically the decoder-only GPT architecture, which is used in every modern LLM.

* **Tokenization**: Understand how to convert raw text data into a format that the model can understand, which involves splitting the text into tokens (usually words or subwords).
* **Attention mechanisms**: Grasp the theory behind attention mechanisms, including self-attention and scaled dot-product attention, which allows the model to focus on different parts of the input when producing an output.
* **Text generation**: Learn about the different ways the model can generate output sequences. Common strategies include greedy decoding, beam search, top-k sampling, and nucleus sampling.

## Hand On LLM architeture

* [Transformer from scratch](./Transformer%20from%20scratch/Transformer%20from%20scratch.ipynb): A detail and intuitive explanation of the Transformer model from scratch.

* [Decoding Strategies in LLMs](./Decoding%20Strategies%20in%20LLMs/Decoding%20Strategies%20in%20LLMs.ipynb): Provide code to different strategies to generate text.


