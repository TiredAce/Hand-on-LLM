{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from scratch\n",
    "\n",
    "In this notebook, we will attempt to oversimplify things a bit and introduce the modules one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    '''Scaled Dot-Product Attention'''\n",
    "\n",
    "    def __init__(self, temperature, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim = -1))\n",
    "        output = torch.matmul(attn, v)\n",
    "        \n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module'''\n",
    "\n",
    "    def __init__(self, n_head, d_in, d_out, \n",
    "                 dropout = 0.1, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert d_out % n_head == 0\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.n_head = n_head\n",
    "        self.d_in = d_in\n",
    "        self.d_head = d_out // n_head\n",
    "\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out = nn.Linear(d_out, d_out, bias = qkv_bias)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=self.d_head ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_out, eps = 1e-6)\n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "        \n",
    "        residual = q\n",
    "\n",
    "        query = self.w_query(q).view(sz_b, len_q, self.n_head, self.d_head)\n",
    "        key = self.w_key(k).view(sz_b, len_k, self.n_head, self.d_head)\n",
    "        value = self.w_value(v).view(sz_b, len_v, self.n_head, self.d_head)\n",
    "\n",
    "        query, key, value = query.transpose(1, 2), key.transpose(1, 2), value.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # for head axis broadcasting\n",
    "\n",
    "        q, attn = self.attention(query, key, value, mask = mask)\n",
    "\n",
    "        x = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        x = self.dropout(self.out(x))\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 8])\n",
      "torch.Size([2, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test MultiHeadAttention \"\"\"\n",
    "\n",
    "mha = MultiHeadAttention(4, 8, 8)\n",
    "\n",
    "q = torch.rand(2, 3, 8)\n",
    "\n",
    "o, attn = mha(q, q, q)\n",
    "print(o.size())\n",
    "print(attn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Wise FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_up = nn.Linear(d_in, d_hid)\n",
    "        self.w_down = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_down(F.relu(self.w_up(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test PositionwiseFeedForward \"\"\"\n",
    "\n",
    "ffn = PositionwiseFeedForward(8, 4 * 8)\n",
    "\n",
    "out = ffn(o)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_out, n_position=200):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('pos_table', self._get_encoding_table(n_position, d_out))\n",
    "\n",
    "    def _get_encoding_table(self, n_position, d_out):\n",
    "\n",
    "        def get_position_angle_vec(position):\n",
    "            return [position / np.power(10000, 2 * (hid_j // 2) / d_out) for hid_j in range(d_out)]\n",
    "        \n",
    "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size()].clone().detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''Compose with two layers'''\n",
    "\n",
    "    def __init__(self, n_head, d_in, d_out, \n",
    "                 dropout = 0.1, qkv_bias = False):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_in, d_out, dropout, qkv_bias)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_in, d_in * 4, dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask = None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask = slf_attn_mask\n",
    "        )\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''A encoder model with self attention mechanism'''\n",
    "\n",
    "    def __init__(\n",
    "            self, src_vocab_size, word_dim, n_layers, n_head, pad_idx, \n",
    "            dropout = 0.1, qkv_bias = False, n_position = 200,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.src_wor_emb = nn.Embedding(src_vocab_size, word_dim, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(word_dim, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(n_head, word_dim, word_dim, dropout, qkv_bias)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(word_dim, eps=1e-6)\n",
    "        \n",
    "    def forward(self, src_seq, src_mask, return_attns = False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        enc_output = self.src_word_emb(src_seq)\n",
    "        enc_output = self.dropout(self.position_enc(enc_output))\n",
    "        enc_output = self.layer_norm(enc_output)\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n",
    "            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attn_list\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''Compose with three layers'''\n",
    "\n",
    "    def __init__(self, n_head, d_in, d_out,\n",
    "                 dropout = 0.1, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_in, d_out, dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_in, d_out, dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_in, d_in * 4, dropout)\n",
    "\n",
    "    def forward(self, dec_input, enc_output, slf_attn_mask=None, \n",
    "                dec_enc_attn_mask = None):\n",
    "        dec_output, dec_slf_attn = self.slf_attn(\n",
    "            dec_input, dec_input, dec_input, mask = slf_attn_mask)\n",
    "        dec_output, dec_enc_attn = self.enc_attn(\n",
    "            dec_output, enc_output, enc_output, mask = dec_enc_attn_mask\n",
    "        )\n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, trg_vocab_size, word_dim, n_layers, n_head, pad_idx,\n",
    "            dropout = 0.1, qkv_bias = False, n_position = 200,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.trg_word_emb = nn.Embedding(trg_vocab_size, word_dim, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(word_dim, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(n_head, word_dim, word_dim, dropout, qkv_bias)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(word_dim, eps=1e-6)\n",
    "        \n",
    "    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns = False):\n",
    "        \n",
    "        dec_slf_attn_list, dec_enc_attn_list = [], []\n",
    "\n",
    "        dec_output = self.trg_word_emb(trg_seq)\n",
    "        dec_output = self.dropout(self.position_enc(dec_output))\n",
    "        dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n",
    "            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_output,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(\n",
    "        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    ''' A sequence to sequence model with attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n",
    "            d_word_vec=512, n_layers=6, n_head=8, dropout=0.1, n_position=200,\n",
    "            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True,\n",
    "            scale_emb_or_prj='prj'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n",
    "\n",
    "        # In section 3.4 of paper \"Attention Is All You Need\", there is such detail:\n",
    "        # \"In our model, we share the same weight matrix between the two\n",
    "        # embedding layers and the pre-softmax linear transformation...\n",
    "        # In the embedding layers, we multiply those weights by \\sqrt{d_model}\".\n",
    "        #\n",
    "        # Options here:\n",
    "        #   'emb': multiply \\sqrt{d_model} to embedding output\n",
    "        #   'prj': multiply (\\sqrt{d_model} ^ -1) to linear projection output\n",
    "        #   'none': no multiplication\n",
    "\n",
    "        assert scale_emb_or_prj in ['emb', 'prj', 'none']\n",
    "        scale_emb = (scale_emb_or_prj == 'emb') if trg_emb_prj_weight_sharing else False\n",
    "        self.scale_prj = (scale_emb_or_prj == 'prj') if trg_emb_prj_weight_sharing else False\n",
    "\n",
    "        self.encoder = Encoder(n_src_vocab, d_word_vec, n_layers, n_head, src_pad_idx, \n",
    "            dropout = 0.1, qkv_bias = False, n_position = 200,)\n",
    "\n",
    "        self.decoder = Decoder(n_trg_vocab, d_word_vec, n_layers, n_head, trg_pad_idx, \n",
    "            dropout = 0.1, qkv_bias = False, n_position = 200,)\n",
    "\n",
    "        self.trg_word_prj = nn.Linear(d_word_vec, n_trg_vocab, bias=False)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) \n",
    "\n",
    "    def forward(self, src_seq, trg_seq):\n",
    "\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
    "        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_mask)\n",
    "        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
    "        seq_logit = self.trg_word_prj(dec_output)\n",
    "        if self.scale_prj:\n",
    "            seq_logit *= self.d_model ** -0.5\n",
    "\n",
    "        return seq_logit.view(-1, seq_logit.size(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "* [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "* [LLM Visualization](https://bbycroft.net/llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
