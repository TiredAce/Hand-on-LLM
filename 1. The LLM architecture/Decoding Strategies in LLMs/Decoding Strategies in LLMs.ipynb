{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Strategies in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from tqdm.notebook import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream of being a doctor.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_path = \"/home/ub/WD8T/cbh/gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "text = \"I have a dream\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=len(input_ids.squeeze())+5, pad_token_id=50256)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2, a autoregressive models, predict the next token in a sequence based on the preceding tokens. Consider a sequence of token $w = (w_1, w_2, ..., w_t)$. The joint probability of this sequence $P(w)$ can be broken down as:\n",
    "\n",
    "$$\n",
    "P(w) = \\prod \\limits^{n}_{i= 1} P(w_i | w_1 ... w_{i - 1})\n",
    "$$\n",
    "\n",
    "For each token $w_i$ in the sequence, $P(w_i | w_1 ... w_{i - 1})$ represents the conditional probability of $w_i$ given all the preceding tokens $w_1 ... w_n$. GPT-2 calculates this conditional probability for each of the tokens in its vocabulary.\n",
    "\n",
    "This leads to the question: how do we use these probabilities to generate next token? There are several decoding strategies, such as greedy search, beam search, Top-K sampling and Nucleus sampling, that come into play.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob(logits, token_id):\n",
    "    # Compute the softmax of the logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    log_probabilities = torch.log(probabilities)\n",
    "    # Get the log probability of the token\n",
    "    token_log_probability = log_probabilities[token_id].item()\n",
    "\n",
    "    return token_log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search\n",
    "\n",
    "Greedy search is a decoding method that takes the most probable token at each step as the next token in the sequence. To put it simply, it only retains the most likely token at each stage, discarding all other potential options.\n",
    "\n",
    "While this approach might sound intuitive, it's important to note that the greedy search is short-sighted, because it only considers the most probable token at each step without considering the overall effort on the sequence. This property makes it fast and efficient as it does not need to keep track of multiple sequences, but it also means that it can miss out on better sequences that might have appeared with slightly less probable next tokens.\n",
    "\n",
    "Now we implement the strategies of Greedy Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: I have a dream of\n",
      "New token score: of -> 15.20%\n",
      "Stage 2: I have a dream of being\n",
      "New token score: being -> 9.68%\n",
      "Stage 3: I have a dream of being a\n",
      "New token score: a -> 30.59%\n",
      "Stage 4: I have a dream of being a doctor\n",
      "New token score: doctor -> 2.86%\n",
      "Stage 5: I have a dream of being a doctor.\n",
      "New token score:. -> 22.93%\n"
     ]
    }
   ],
   "source": [
    "def greedy_search(model, input_ids):\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits  # shape = ([1, 4, 50257])\n",
    "\n",
    "    # Using the last token outputs to predict next sub-word\n",
    "    logits = predictions[0, -1, :]\n",
    "    \n",
    "    token_id = torch.argmax(logits).unsqueeze(0)\n",
    "\n",
    "    # Compute the score of the predicted token\n",
    "    log_token_score = get_log_prob(logits, token_id)\n",
    "\n",
    "    token_score = math.exp(log_token_score)\n",
    "\n",
    "    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
    "\n",
    "    return new_input_ids, token_score\n",
    "\n",
    "new_input_ids = input_ids\n",
    "num_of_generate_token = 5\n",
    "\n",
    "for i in range(1, num_of_generate_token + 1):\n",
    "    new_input_ids, token_score = greedy_search(model, new_input_ids)\n",
    "    print(f\"Stage {i}: {tokenizer.decode(new_input_ids[0])}\")\n",
    "    print(f\"New token score:{tokenizer.decode(new_input_ids[0, -1])} -> {token_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you can see the output of our implement of greedy search is similar to the original output of GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "\n",
    "Unlike greedy search, which only consider the next most probable token, beam search takes into account the $n$ most likely tokens, where $n$ presents the number of beams. This procedure is repeated until a predefine maximum length is reached or an end-of-sequence token appears. At this point, the sequence (or \"beam\") with the highest overall score is chosen as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the output of the original GPT-2 model when using beam search to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream. I have a dream\n"
     ]
    }
   ],
   "source": [
    "# Using the generate function and setting the parameter of num_beams as 5\n",
    "outputs = model.generate(input_ids, max_length=len(input_ids.squeeze())+5, num_beams = 4, pad_token_id = 50256)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, you can see the model generated the sentence 'I have a dream. I want to be.' Now, let's implement our own beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1 place of generation text: I have a dream. I have a dream\n",
      "The cumulative score of the sentence: -6.252\n",
      "The 2 place of generation text: I have a dream. I want to be\n",
      "The cumulative score of the sentence: -6.674\n",
      "The 3 place of generation text: I have a dream. I want to go\n",
      "The cumulative score of the sentence: -7.928\n"
     ]
    }
   ],
   "source": [
    "def greedy_sampling(logits, beams):\n",
    "    k = torch.topk(logits, beams)\n",
    "    \n",
    "    return k.indices\n",
    "\n",
    "def beam_search(model, inputs_ids, max_length = 5, num_beams = 2, sampling = greedy_sampling):\n",
    "    beams = [(inputs_ids, 0)]  # (sequence, score)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # store the next beams\n",
    "        paticipate = []\n",
    "        for sequence, score in beams:\n",
    "            output = model(sequence)\n",
    "            prediction = output.logits\n",
    "\n",
    "            logits = prediction[0, -1, :]\n",
    "\n",
    "            top_k_token = sampling(logits, num_beams)\n",
    "\n",
    "            for next_token in top_k_token.unsqueeze(1):\n",
    "                new_input_ids = torch.cat([sequence, next_token.unsqueeze(0)], dim = -1)\n",
    "                new_token_score = get_log_prob(logits, next_token)\n",
    "                paticipate.append((new_input_ids, score + new_token_score))\n",
    "\n",
    "        beams = paticipate\n",
    "\n",
    "    sorted_beams = sorted(beams, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    return sorted_beams\n",
    "\n",
    "new_input_ids = input_ids\n",
    "# Parameters\n",
    "num_of_generate_token = 5\n",
    "beams = 2\n",
    "\n",
    "best_sequence = beam_search(model, new_input_ids, num_of_generate_token, beams)\n",
    "\n",
    "for i in range(3):\n",
    "    generated_text = tokenizer.decode(best_sequence[i][0].squeeze(0), skip_special_tokens=True)\n",
    "    print(f\"The {i + 1} place of generation text: {generated_text}\\nThe cumulative score of the sentence: {best_sequence[i][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-k sampling\n",
    "\n",
    "Top-k sampling is a technique that leverages the probability distribution generated by the language model to select **a token randomly from the K most likely options**.\n",
    "\n",
    "Another way of introducing randomness is the concept of temperature. The temperature $T$ is a parameter that ranges from $0$ to $1$, which affects the probabilities generated by the softmax function, making the most likely tokens more influential. In pratice, it simply consists of dividing the input logits by a value we call temperature:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i / T}}{\\sum_j e^{x_j/T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, let's now implement the top k sampling algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1 place of generation text: I have a dream job and I want to\n",
      "The cumulative score of the sentence: -9.102\n",
      "The 2 place of generation text: I have a dream job.\n",
      "\n",
      "\"\n",
      "The cumulative score of the sentence: -10.815\n",
      "The 3 place of generation text: I have a dream for the future.\"\n",
      "\n",
      "The cumulative score of the sentence: -10.928\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(logits, beams, temperature = 1, top_k = 20):\n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][-1]\n",
    "    new_logits = torch.clone(logits)\n",
    "    new_logits[indices_to_remove] = float(\"-inf\")\n",
    "\n",
    "    prob = torch.nn.functional.softmax(new_logits / temperature, dim = -1)\n",
    "\n",
    "    next_tokens = torch.multinomial(prob, beams)\n",
    "\n",
    "    return next_tokens\n",
    "\n",
    "new_input_ids = input_ids\n",
    "# Parameters\n",
    "num_of_generate_token = 5\n",
    "beams = 2\n",
    "\n",
    "best_sequence = beam_search(model, new_input_ids, num_of_generate_token, beams, top_k_sampling)\n",
    "\n",
    "for i in range(3):\n",
    "    generated_text = tokenizer.decode(best_sequence[i][0].squeeze(0), skip_special_tokens=True)\n",
    "    print(f\"The {i + 1} place of generation text: {generated_text}\\nThe cumulative score of the sentence: {best_sequence[i][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleus sampling\n",
    "\n",
    "Nucleus sampling, also known as top-p sampling, taks a different approach from top-k sampling. Rather then selecting the $k$ most probable tokens, nucleus sampling chooses a cutoff value $p$ such that the **sum of the probabilities of the selected tokens exceeds $p$**. This forms a \"nucleus\" of tokens from which to randomly choose the next token.\n",
    "\n",
    "This variability often results in a more diverse and creative output, making nucleus sampling popular for tasks such as text generation.\n",
    "\n",
    "Now we implement our nucleus sampling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1 place of generation text: I have a dream. That is my dream. It is my dream. I\n",
      "The cumulative score of the sentence: -20.510\n",
      "The 2 place of generation text: I have a dream. That is my dream. It is my dream.\n",
      "\n",
      "The cumulative score of the sentence: -21.389\n",
      "The 3 place of generation text: I have a dream. That is my dream.\n",
      "\n",
      "\"I don't\n",
      "The cumulative score of the sentence: -21.672\n"
     ]
    }
   ],
   "source": [
    "def nucleus_sampling(logits, beams, temperature = 1.0, p = 0.7):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    prob = torch.nn.functional.softmax(sorted_logits / temperature, dim = -1)\n",
    "    cum_prob = torch.cumsum(prob, dim = -1)\n",
    "\n",
    "    mask = cum_prob < p\n",
    "\n",
    "    if mask.sum() > beams:\n",
    "        top_p_index_to_keep = torch.where(mask)[0][-1].detach().cpu().tolist()\n",
    "    else:\n",
    "        top_p_index_to_keep = beams\n",
    "\n",
    "    indices_to_remove = sorted_indices[top_p_index_to_keep:]\n",
    "    sorted_logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n",
    "    next_tokens = torch.multinomial(probabilities, beams)\n",
    "\n",
    "    return next_tokens\n",
    "\n",
    "new_input_ids = input_ids\n",
    "# Parameters\n",
    "num_of_generate_token = 12\n",
    "beams = 2\n",
    "\n",
    "best_sequence = beam_search(model, new_input_ids, num_of_generate_token, beams, nucleus_sampling)\n",
    "\n",
    "for i in range(3):\n",
    "    generated_text = tokenizer.decode(best_sequence[i][0].squeeze(0), skip_special_tokens=True)\n",
    "    print(f\"The {i + 1} place of generation text: {generated_text}\\nThe cumulative score of the sentence: {best_sequence[i][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this article, we have delved deep into various decoding methods used by LLMs, specifically GPT-2. Understanding these techniques and their trade-offs will equip you to better guide the LLMs towards producing increasingly realistic, nuanced, and compelling textual output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "* [Decoding Strategies in Large Language Models](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)\n",
    "* [Beam Search](https://medium.com/@jingying.h95/beam-search-1e875fd59ab4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
